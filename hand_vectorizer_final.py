import numpy as np
import pandas as pd
import cv2
import mediapipe as mp
from pathlib import Path
import pickle
from hand_vectorizer import HandVectorizer

class OptimizedHandVectorizer(HandVectorizer):
    """
    Vectorizador de manos optimizado con normalizaci√≥n robusta
    para ser invariante a escala y distancia de la c√°mara
    """
    
    def __init__(self):
        super().__init__()
        self.reference_data = None
        self.feature_scaler = None
        
    def normalize_features_robust(self, features):
        """
        Normalizaci√≥n robusta que hace las caracter√≠sticas invariantes a escala
        """
        features = np.array(features)
        
        # 1. Separar coordenadas de landmarks (63) y distancias (5)
        landmark_coords = features[:63]  # x,y,z de 21 landmarks
        distances = features[63:68] if len(features) >= 68 else features[63:]
        
        # 2. Normalizar coordenadas de landmarks por escala
        # Reshape a (21, 3) para procesar cada landmark
        landmarks_3d = landmark_coords.reshape(-1, 3)
        
        # Calcular el centro de masa de la mano
        center = np.mean(landmarks_3d, axis=0)
        
        # Centrar los landmarks
        centered_landmarks = landmarks_3d - center
        
        # Calcular la escala basada en la distancia m√°xima desde el centro
        max_distance = np.max(np.linalg.norm(centered_landmarks, axis=1))
        
        # Normalizar por escala (hacer la mano de tama√±o unitario)
        if max_distance > 0:
            normalized_landmarks = centered_landmarks / max_distance
        else:
            normalized_landmarks = centered_landmarks
        
        # 3. Normalizar distancias relativas
        if len(distances) > 0:
            # Calcular distancias relativas al tama√±o de la mano
            if max_distance > 0:
                normalized_distances = distances / max_distance
            else:
                normalized_distances = distances
        else:
            normalized_distances = np.array([])
        
        # 4. Reconstruir vector de caracter√≠sticas
        normalized_features = np.concatenate([
            normalized_landmarks.flatten(),
            normalized_distances
        ])
        
        # 5. Asegurar que siempre tengamos 68 caracter√≠sticas
        while len(normalized_features) < 68:
            normalized_features = np.append(normalized_features, 0.0)
        
        if len(normalized_features) > 68:
            normalized_features = normalized_features[:68]
        
        return normalized_features
    
    def extract_scale_invariant_features(self, landmarks_data):
        """
        Extrae caracter√≠sticas invariantes a escala de los landmarks
        """
        if not landmarks_data or not landmarks_data['landmarks']:
            return None
        
        landmarks = landmarks_data['landmarks']
        
        # Obtener caracter√≠sticas b√°sicas
        basic_features = self.get_landmark_features(landmarks_data)
        if basic_features is None:
            return None
        
        # Aplicar normalizaci√≥n robusta
        normalized_features = self.normalize_features_robust(basic_features)
        
        return normalized_features
    
    def load_reference_dataset_optimized(self, dataset_path="kagglehub/asl_dataset"):
        """
        Carga el dataset de referencia con caracter√≠sticas normalizadas
        """
        print("üìä Cargando dataset de referencia optimizado...")
        
        # Verificar si existe el archivo de caracter√≠sticas procesadas
        processed_file = "reference_features_optimized.pkl"
        
        if Path(processed_file).exists():
            print("‚úÖ Cargando caracter√≠sticas pre-procesadas...")
            with open(processed_file, 'rb') as f:
                self.reference_data = pickle.load(f)
            print(f"‚úÖ Dataset cargado con {len(self.reference_data)} clases")
            return
        
        # Si no existe, procesar el dataset
        print("üîÑ Procesando dataset para extraer caracter√≠sticas optimizadas...")
        
        self.reference_data = {}
        dataset_path = Path(dataset_path)
        
        # Procesar cada clase
        for class_folder in dataset_path.iterdir():
            if class_folder.is_dir():
                class_name = class_folder.name
                
                # Solo procesar clases v√°lidas
                if class_name in [str(i) for i in range(10)] + [chr(i) for i in range(97, 123)]:
                    print(f"  Procesando clase: {class_name}")
                    
                    class_features = []
                    
                    # Procesar im√°genes de la clase
                    for image_file in class_folder.glob("*.jpeg"):
                        landmarks_data = self.extract_hand_landmarks(str(image_file))
                        
                        if landmarks_data and landmarks_data['landmarks']:
                            features = self.extract_scale_invariant_features(landmarks_data)
                            if features is not None:
                                class_features.append(features)
                    
                    if class_features:
                        # Calcular caracter√≠sticas promedio de la clase
                        avg_features = np.mean(class_features, axis=0)
                        self.reference_data[class_name] = avg_features
                        print(f"    ‚úÖ {len(class_features)} im√°genes procesadas")
                    else:
                        print(f"    ‚ùå No se pudieron extraer caracter√≠sticas")
        
        # Guardar caracter√≠sticas procesadas
        print("üíæ Guardando caracter√≠sticas optimizadas...")
        with open(processed_file, 'wb') as f:
            pickle.dump(self.reference_data, f)
        
        print(f"‚úÖ Dataset optimizado cargado con {len(self.reference_data)} clases")
        print(f"üìÅ Caracter√≠sticas guardadas en: {processed_file}")
    
    def recognize_gesture_optimized(self, landmarks_data):
        """
        Reconoce gestos con caracter√≠sticas optimizadas y normalizadas
        """
        if not landmarks_data or not landmarks_data['landmarks']:
            return "‚ùå No se detect√≥ mano"
        
        if self.reference_data is None:
            return "‚ùå Dataset no cargado"
        
        # Extraer caracter√≠sticas optimizadas
        current_features = self.extract_scale_invariant_features(landmarks_data)
        if current_features is None:
            return "‚ùå No se pudieron extraer caracter√≠sticas"
        
        # Comparar con cada clase de referencia
        best_match = None
        best_score = -1
        top_matches = []
        
        for class_name, ref_features in self.reference_data.items():
            if len(ref_features) == len(current_features):
                # Calcular similitud de coseno (invariante a escala)
                cosine_similarity = np.dot(current_features, ref_features) / (
                    np.linalg.norm(current_features) * np.linalg.norm(ref_features)
                )
                
                # Calcular correlaci√≥n
                correlation = np.corrcoef(current_features, ref_features)[0, 1]
                if np.isnan(correlation):
                    correlation = 0
                
                # Score combinado (ponderado hacia similitud de coseno)
                combined_score = (cosine_similarity * 0.7 + correlation * 0.3)
                
                top_matches.append({
                    'class': class_name,
                    'cosine_similarity': cosine_similarity,
                    'correlation': correlation,
                    'combined_score': combined_score
                })
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_match = class_name
        
        # Ordenar por score
        top_matches.sort(key=lambda x: x['combined_score'], reverse=True)
        
        # Mostrar top 3 coincidencias (menos verbose)
        print("\nüîç Top 3 coincidencias:")
        for i, match in enumerate(top_matches[:3]):
            confidence = max(0, match['combined_score'])
            description = self.get_gesture_description(match['class'])
            print(f"   {i+1}. {description}: {confidence:.1%}")
        
        # Determinar confianza y respuesta
        if best_match:
            confidence = max(0, best_score)
            description = self.get_gesture_description(best_match)
            
            if confidence > 0.75:  # Umbral alto para alta confianza
                return f"‚úÖ {description} ({confidence:.1%})"
            elif confidence > 0.60:  # Umbral medio
                return f"ü§î {description} ({confidence:.1%})"
            elif confidence > 0.45:  # Umbral bajo
                return f"‚ùì {description} ({confidence:.1%})"
            else:
                return f"‚ùì Posible {description} ({confidence:.1%})"
        else:
            return "‚ùå Se√±a no reconocida"
    
    def run_camera_optimized(self):
        """
        C√°mara optimizada con reconocimiento robusto
        """
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå No se pudo abrir la c√°mara.")
            return
        
        # Configurar resoluci√≥n
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        # Crear instancia optimizada para video
        video_hands = self.create_video_hands()
        
        print("üé• C√°mara optimizada activada. Pulsa 'q' para salir.")
        print("üñêÔ∏è  Sistema optimizado para funcionar a cualquier distancia")
        print("=" * 60)
        
        last_recognition = ""
        frame_count = 0
        recognition_interval = 20  # Reconocer cada 20 frames (m√°s estable)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = video_hands.process(image_rgb)
            
            # Dibujar landmarks
            if results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    self.mp_drawing.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                
                # Reconocer gesto
                if frame_count % recognition_interval == 0:
                    landmarks_data = {
                        'landmarks': []
                    }
                    for landmark in results.multi_hand_landmarks[0].landmark:
                        landmarks_data['landmarks'].append({
                            'x': landmark.x,
                            'y': landmark.y,
                            'z': landmark.z
                        })
                    
                    recognition = self.recognize_gesture_optimized(landmarks_data)
                    
                    if recognition != last_recognition:
                        print(f"üñêÔ∏è  {recognition}")
                        last_recognition = recognition
            else:
                # Resetear reconocimiento si no hay manos
                if last_recognition != "":
                    last_recognition = ""
            
            # Mostrar informaci√≥n en pantalla
            cv2.putText(frame, "Sistema Optimizado - Cualquier distancia", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            if results.multi_hand_landmarks:
                cv2.putText(frame, f"Manos: {len(results.multi_hand_landmarks)}", (10, 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                cv2.putText(frame, f"Frame: {frame_count}", (10, 90), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            else:
                cv2.putText(frame, "No se detectan manos", (10, 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            
            cv2.imshow('Sistema Optimizado - Presiona q para salir', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            frame_count += 1
        
        cap.release()
        cv2.destroyAllWindows()
        video_hands.close()
        print("üñêÔ∏è  C√°mara cerrada.")

def main():
    """
    Funci√≥n principal del sistema optimizado
    """
    print("üöÄ SISTEMA DE RECONOCIMIENTO DE SE√ëAS OPTIMIZADO")
    print("=" * 60)
    print("‚ú® Caracter√≠sticas:")
    print("   ‚Ä¢ Normalizaci√≥n invariante a escala")
    print("   ‚Ä¢ Funciona a cualquier distancia de la c√°mara")
    print("   ‚Ä¢ Reconocimiento m√°s estable")
    print("   ‚Ä¢ Menos sensible a variaciones de posici√≥n")
    print("=" * 60)
    
    # Crear vectorizador optimizado
    vectorizer = OptimizedHandVectorizer()
    
    # Cargar dataset optimizado
    vectorizer.load_reference_dataset_optimized()
    
    # Mostrar clases disponibles
    if vectorizer.reference_data:
        classes = sorted(vectorizer.reference_data.keys())
        print(f"\nüìö Clases disponibles ({len(classes)}):")
        print("   N√∫meros: " + ", ".join([c for c in classes if c.isdigit()]))
        print("   Letras: " + ", ".join([c for c in classes if c.isalpha()]))
    
    print("\nüéØ Iniciando c√°mara optimizada...")
    print("üí° Consejos:")
    print("   ‚Ä¢ Funciona a cualquier distancia de la c√°mara")
    print("   ‚Ä¢ Mant√©n la mano bien iluminada")
    print("   ‚Ä¢ Haz las se√±as de forma clara y pausada")
    print("   ‚Ä¢ El sistema es m√°s estable que antes")
    
    # Iniciar c√°mara
    vectorizer.run_camera_optimized()

if __name__ == "__main__":
    main() 